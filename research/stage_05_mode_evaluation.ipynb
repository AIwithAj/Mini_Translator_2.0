{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%pwd\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class model_eval_config:\n",
    "    root_dir: Path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Mini_Translator_T.constants import *\n",
    "from src.Mini_Translator_T.utils.common import read_yaml, create_directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_eval_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mConfigurationManager\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m      4\u001b[0m         config_filepath \u001b[38;5;241m=\u001b[39m CONFIG_FILE_PATH,\n\u001b[0;32m      5\u001b[0m         params_filepath \u001b[38;5;241m=\u001b[39m PARAMS_FILE_PATH):\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m read_yaml(config_filepath)\n",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m, in \u001b[0;36mConfigurationManager\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m read_yaml(params_filepath)\n\u001b[0;32m     10\u001b[0m     create_directories([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39martifacts_root])\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_eval_model_config\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mmodel_eval_config\u001b[49m:\n\u001b[0;32m     15\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel_evaluation\n\u001b[0;32m     16\u001b[0m     params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_eval_config' is not defined"
     ]
    }
   ],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_eval_model_config(self) -> model_eval_config:\n",
    "        config = self.config.model_evaluation\n",
    "        params=self.params\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        eval_config=model_eval_config(root_dir=config.root_dir)\n",
    "        \n",
    "        return eval_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-24 11:10:42,542: INFO: config: PyTorch version 2.0.0 available.]\n",
      "[2024-05-24 11:10:42,554: INFO: config: TensorFlow version 2.16.1 available.]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from src.Mini_Translator_T.logging import logger\n",
    "import pandas as pd\n",
    "from src.Mini_Translator_T.config.configuration import model_trainer_config\n",
    "from src.Mini_Translator_T.utils.common import casual_mask\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    def __init__(self,config:model_eval_config,config2=model_trainer_config):\n",
    "\n",
    "        \n",
    "        self.config=config\n",
    "        self.config2=config2\n",
    "\n",
    "\n",
    "\n",
    "    # Define function to obtain the most probable next token\n",
    "    def greedy_decode(self,model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "        # Retrieving the indices from the start and end of sequences of the target tokens\n",
    "        sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "        eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "        # Computing the output of the encoder for the source sequence\n",
    "        encoder_output = model.encode(source, source_mask)\n",
    "        # Initializing the decoder input with the Start of Sentence token\n",
    "        decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
    "\n",
    "        # Looping until the 'max_len', maximum length, is reached\n",
    "        while True:\n",
    "            if decoder_input.size(1) == max_len:\n",
    "                break\n",
    "\n",
    "            # Building a mask for the decoder input\n",
    "            decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "            # Calculating the output of the decoder\n",
    "            out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "            # Applying the projection layer to get the probabilities for the next token\n",
    "            prob = model.project(out[:, -1])\n",
    "\n",
    "            # Selecting token with the highest probability\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
    "\n",
    "            # If the next token is an End of Sentence token, we finish the loop\n",
    "            if next_word == eos_idx:\n",
    "                break\n",
    "\n",
    "        return decoder_input.squeeze(0) # Sequence of tokens generated by the decoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Defining function to evaluate the model on the validation dataset\n",
    "    # num_examples = 2, two examples per run\n",
    "    def run_validation(self,model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device num_examples=2):\n",
    "        model.eval() # Setting model to evaluation mode\n",
    "        count = 0 # Initializing counter to keep track of how many examples have been processed\n",
    "\n",
    "        console_width = 80 # Fixed witdh for printed messages\n",
    "\n",
    "        # Creating evaluation loop\n",
    "        with torch.no_grad(): # Ensuring that no gradients are computed during this process\n",
    "            for batch in validation_ds:\n",
    "                count += 1\n",
    "                encoder_input = batch['encoder_input'].to(device)\n",
    "                encoder_mask = batch['encoder_mask'].to(device)\n",
    "\n",
    "                # Ensuring that the batch_size of the validation set is 1\n",
    "                assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n",
    "\n",
    "                # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n",
    "                model_out = self.greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "                # Retrieving source and target texts from the batch\n",
    "                source_text = batch['src_text'][0]\n",
    "                target_text = batch['tgt_text'][0] # True translation\n",
    "                model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n",
    "\n",
    "                # Printing results\n",
    "                logger.info('-'*console_width)\n",
    "                logger.info(f'SOURCE: {source_text}')\n",
    "                logger.info(f'TARGET: {target_text}')\n",
    "                logger.info(f'PREDICTED: {model_out_text}')\n",
    "\n",
    "                # After two examples, we break the loop\n",
    "                if count == num_examples:\n",
    "                    break\n",
    "\n",
    "\n",
    "    def initiate_model_trainer(self):\n",
    "        config=self.config2\n",
    "        root_dir = \"artifacts/data_transformation\"\n",
    "        valid_data_loader_path = os.path.join(root_dir, \"valid_data_loader.pth\")\n",
    "\n",
    "        \n",
    "        tokenizer_en = os.path.join(root_dir, \"'tokenizer_en.json'\")\n",
    "        tokenizer_it = os.path.join(root_dir, \"'tokenizer_it.json'\")\n",
    "        tokenizer_src = Tokenizer.from_file(tokenizer_en)\n",
    "        tokenizer_tgt = Tokenizer.from_file(tokenizer_it)\n",
    "\n",
    "\n",
    "\n",
    "        # Load the DataLoader objects\n",
    "        valid_data_loader = torch.load(valid_data_loader_path)\n",
    "      \n",
    "\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        logger.info(f\"Using device {device}\")\n",
    "\n",
    "        # Creating model directory to store weights\n",
    "        Path(config.model_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Retrieving dataloaders and tokenizers for source and target languages using the 'get_ds' function\n",
    "\n",
    "        # Initializing model on the GPU using the 'get_model' function\n",
    "        # Load the entire model\n",
    "        model = torch.load('full_model.pth')\n",
    "\n",
    "\n",
    "        self.run_validation(model, valid_data_loader, tokenizer_src, tokenizer_tgt, config.seq_len,device)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-24 11:19:16,181: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-05-24 11:19:16,191: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-05-24 11:19:16,196: INFO: common: created directory at: artifacts]\n",
      "[2024-05-24 11:19:16,201: INFO: common: created directory at: artifacts/evaluation]\n",
      "[2024-05-24 11:19:16,211: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "| Test Loss: 5.823 | Test PPL: 337.897 |\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'WindowsPath' object has no attribute 'data_transformation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39minitiate_model_Evaluation()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     eval_model_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_eval_model_config()\n\u001b[0;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m Evaluation(config\u001b[38;5;241m=\u001b[39meval_model_config)\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_model_Evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[10], line 93\u001b[0m, in \u001b[0;36mEvaluation.initiate_model_Evaluation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     88\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_fn(model, test_data_loader, criterion, device)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Test PPL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mexp(test_loss)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m7.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m |\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m en_nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_transformation\u001b[49m\u001b[38;5;241m.\u001b[39mtokenizer_1)\n\u001b[0;32m     94\u001b[0m de_nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig2\u001b[38;5;241m.\u001b[39mdata_transformation\u001b[38;5;241m.\u001b[39mtokenizer_2)\n\u001b[0;32m     97\u001b[0m en_vocab \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_vocab.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WindowsPath' object has no attribute 'data_transformation'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    eval_model_config = config.get_eval_model_config()\n",
    "    model = Evaluation(config=eval_model_config)\n",
    "    model.initiate_model_Evaluation()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
