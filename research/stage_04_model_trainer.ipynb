{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%pwd\n",
    "os.chdir(\"../\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class model_trainer_config:\n",
    "    root_dir: Path\n",
    "    seq_len:int\n",
    "    num_epochs: int\n",
    "    model_folder: str\n",
    "    model_basename:str\n",
    "    preload: None\n",
    "    experiment_name: str\n",
    "    lr:float\n",
    "    d_model:int\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Mini_Translator_T.constants import *\n",
    "from src.Mini_Translator_T.utils.common import read_yaml, create_directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_train_model_config(self) -> model_trainer_config:\n",
    "        config = self.config.model_trainer\n",
    "        params=self.params\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        trainer_config=model_trainer_config(root_dir=config.root_dir,seq_len=params.seq_len,num_epochs=params.num_epochs,\n",
    "                                            model_folder=config.model_folder,model_basename=config.model_basename,\n",
    "                                            experiment_name=config.experiment_name,lr=params.lr,d_model=params.d_model,preload=None)\n",
    "        \n",
    "        return trainer_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from src.Mini_Translator_T.logging import logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Mini_Translator_T import (Transformer,InputEmbeddings,PositionalEncoding,\n",
    "MultiHeadAttentionBlock,FeedForwardBlock,EncoderBlock,DecoderBlock,Encoder,Decoder,ProjectionLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casual_mask(size):\n",
    "        # Creating a square matrix of dimensions 'size x size' filled with ones\n",
    "        mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n",
    "        return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to obtain the most probable next token\n",
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    # Retrieving the indices from the start and end of sequences of the target tokens\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Computing the output of the encoder for the source sequence\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initializing the decoder input with the Start of Sentence token\n",
    "    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
    "\n",
    "    # Looping until the 'max_len', maximum length, is reached\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # Building a mask for the decoder input\n",
    "        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # Calculating the output of the decoder\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # Applying the projection layer to get the probabilities for the next token\n",
    "        prob = model.project(out[:, -1])\n",
    "\n",
    "        # Selecting token with the highest probability\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
    "\n",
    "        # If the next token is an End of Sentence token, we finish the loop\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0) # Sequence of tokens generated by the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to evaluate the model on the validation dataset\n",
    "# num_examples = 2, two examples per run\n",
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n",
    "    model.eval() # Setting model to evaluation mode\n",
    "    count = 0 # Initializing counter to keep track of how many examples have been processed\n",
    "\n",
    "    console_width = 80 # Fixed witdh for printed messages\n",
    "\n",
    "    # Creating evaluation loop\n",
    "    with torch.no_grad(): # Ensuring that no gradients are computed during this process\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "\n",
    "            # Ensuring that the batch_size of the validation set is 1\n",
    "            assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n",
    "\n",
    "            # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            # Retrieving source and target texts from the batch\n",
    "            source_text = batch['src_text'][0]\n",
    "            target_text = batch['tgt_text'][0] # True translation\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n",
    "\n",
    "            # Printing results\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f'SOURCE: {source_text}')\n",
    "            print_msg(f'TARGET: {target_text}')\n",
    "            print_msg(f'PREDICTED: {model_out_text}')\n",
    "\n",
    "            # After two examples, we break the loop\n",
    "            if count == num_examples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelTrainer:\n",
    "    def __init__(self,config:model_trainer_config,config_filepath = CONFIG_FILE_PATH):\n",
    "        self.config=config\n",
    "        self.config2=config_filepath\n",
    "\n",
    "\n",
    "    # Building & Initializing Transformer\n",
    "\n",
    "    def get_weights_file_path(self,config, epoch: str):\n",
    "        config=self.config\n",
    "        model_folder = config.model_folder # Extracting model folder from the config\n",
    "        model_basename = config.model_basename # Extracting the base name for model files\n",
    "        model_filename = f\"{model_basename}{epoch}.pt\" # Building filename\n",
    "        return str(Path('.')/ model_folder/ model_filename) # Combining current directory, the model folder, and the model filename\n",
    "\n",
    "# Definin function and its parameter, including model dimension, number of encoder and decoder stacks, heads, etc.\n",
    "    def build_transformer(self,src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
    "\n",
    "        # Creating Embedding layers\n",
    "        src_embed = InputEmbeddings(d_model, src_vocab_size) # Source language (Source Vocabulary to 512-dimensional vectors)\n",
    "        tgt_embed = InputEmbeddings(d_model, tgt_vocab_size) # Target language (Target Vocabulary to 512-dimensional vectors)\n",
    "\n",
    "        # Creating Positional Encoding layers\n",
    "        src_pos = PositionalEncoding(d_model, src_seq_len, dropout) # Positional encoding for the source language embeddings\n",
    "        tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout) # Positional encoding for the target language embeddings\n",
    "\n",
    "        # Creating EncoderBlocks\n",
    "        encoder_blocks = [] # Initial list of empty EncoderBlocks\n",
    "        for _ in range(N): # Iterating 'N' times to create 'N' EncoderBlocks (N = 6)\n",
    "            encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Self-Attention\n",
    "            feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n",
    "\n",
    "            # Combine layers into an EncoderBlock\n",
    "            encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "            encoder_blocks.append(encoder_block) # Appending EncoderBlock to the list of EncoderBlocks\n",
    "\n",
    "        # Creating DecoderBlocks\n",
    "        decoder_blocks = [] # Initial list of empty DecoderBlocks\n",
    "        for _ in range(N): # Iterating 'N' times to create 'N' DecoderBlocks (N = 6)\n",
    "            decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Self-Attention\n",
    "            decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Cross-Attention\n",
    "            feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n",
    "\n",
    "            # Combining layers into a DecoderBlock\n",
    "            decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "            decoder_blocks.append(decoder_block) # Appending DecoderBlock to the list of DecoderBlocks\n",
    "\n",
    "        # Creating the Encoder and Decoder by using the EncoderBlocks and DecoderBlocks lists\n",
    "        encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "        decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "        # Creating projection layer\n",
    "        projection_layer = ProjectionLayer(d_model, tgt_vocab_size) # Map the output of Decoder to the Target Vocabulary Space\n",
    "\n",
    "        # Creating the transformer by combining everything above\n",
    "        transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "        # Initialize the parameters\n",
    "        for p in transformer.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "        return transformer # Assembled and initialized Transformer. Ready to be trained and validated!\n",
    "    # We pass as parameters the config dictionary, the length of the vocabylary of the source language and the target language\n",
    "    def get_model(self,config, vocab_src_len, vocab_tgt_len):\n",
    "\n",
    "        # Loading model using the 'build_transformer' function.\n",
    "        # We will use the lengths of the source language and target language vocabularies, the 'seq_len', and the dimensionality of the embeddings\n",
    "        model = self.build_transformer(vocab_src_len, vocab_tgt_len, config.seq_len, config.seq_len, config.d_model)\n",
    "        return model\n",
    "\n",
    "# Function to construct the path for saving and retrieving model weights\n",
    "    def get_weights_file_path(self,config, epoch: str):\n",
    "        model_folder = config['model_folder'] # Extracting model folder from the config\n",
    "        model_basename = config['model_basename'] # Extracting the base name for model files\n",
    "        model_filename = f\"{model_basename}{epoch}.pt\" # Building filename\n",
    "        return str(Path('.')/ model_folder/ model_filename) # Combining current directory, the model folder, and the model filename\n",
    "    \n",
    "\n",
    "    def initiate_model_trainer(self):\n",
    "        config=self.config\n",
    "        root_dir = \"artifacts/data_transformation\"\n",
    "        train_data_loader_path = os.path.join(root_dir, \"train_data_loader.pth\")\n",
    "        valid_data_loader_path = os.path.join(root_dir, \"valid_data_loader.pth\")\n",
    "\n",
    "        \n",
    "        tokenizer_en = os.path.join(root_dir, \"'tokenizer_en.json'\")\n",
    "        tokenizer_it = os.path.join(root_dir, \"'tokenizer_it.json'\")\n",
    "        tokenizer_src = Tokenizer.from_file(tokenizer_en)\n",
    "        tokenizer_tgt = Tokenizer.from_file(tokenizer_it)\n",
    "\n",
    "\n",
    "\n",
    "        # Load the DataLoader objects\n",
    "        train_data_loader = torch.load(train_data_loader_path)\n",
    "        valid_data_loader = torch.load(valid_data_loader_path)\n",
    "      \n",
    "\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        logger.info(f\"Using device {device}\")\n",
    "\n",
    "        # Creating model directory to store weights\n",
    "        Path(config.model_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Retrieving dataloaders and tokenizers for source and target languages using the 'get_ds' function\n",
    "\n",
    "        # Initializing model on the GPU using the 'get_model' function\n",
    "        model = self.get_model(config,tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "\n",
    "\n",
    "            # Tensorboard\n",
    "        writer = SummaryWriter(config.experiment_name)\n",
    "\n",
    "        # Setting up the Adam optimizer with the specified learning rate from the '\n",
    "        # config' dictionary plus an epsilon value\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, eps = 1e-9)\n",
    "\n",
    "        # Initializing epoch and global step variables\n",
    "        initial_epoch = 0\n",
    "        global_step = 0\n",
    "\n",
    "        # Checking if there is a pre-trained model to load\n",
    "        # If true, loads it\n",
    "        if config.preload:\n",
    "            model_filename = self.get_weights_file_path(config, config.preload)\n",
    "            print(f'Preloading model {model_filename}')\n",
    "            state = torch.load(model_filename) # Loading model\n",
    "\n",
    "            # Sets epoch to the saved in the state plus one, to resume from where it stopped\n",
    "            initial_epoch = state['epoch'] + 1\n",
    "            # Loading the optimizer state from the saved model\n",
    "            optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "            # Loading the global step state from the saved model\n",
    "            global_step = state['global_step']\n",
    "\n",
    "        # Initializing CrossEntropyLoss function for training\n",
    "        # We ignore padding tokens when computing loss, as they are not relevant for the learning process\n",
    "        # We also apply label_smoothing to prevent overfitting\n",
    "        loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'), label_smoothing = 0.1).to(device)\n",
    "\n",
    "        # Initializing training loop\n",
    "\n",
    "        # Iterating over each epoch from the 'initial_epoch' variable up to\n",
    "        # the number of epochs informed in the config\n",
    "        for epoch in range(initial_epoch, config.num_epochs):\n",
    "\n",
    "            # Initializing an iterator over the training dataloader\n",
    "            # We also use tqdm to display a progress bar\n",
    "            batch_iterator = tqdm(train_data_loader, desc = f'Processing epoch {epoch:02d}')\n",
    "\n",
    "            # For each batch...\n",
    "            for batch in batch_iterator:\n",
    "                model.train() # Train the model\n",
    "\n",
    "                # Loading input data and masks onto the GPU\n",
    "                encoder_input = batch['encoder_input'].to(device)\n",
    "                decoder_input = batch['decoder_input'].to(device)\n",
    "                encoder_mask = batch['encoder_mask'].to(device)\n",
    "                decoder_mask = batch['decoder_mask'].to(device)\n",
    "\n",
    "                # Running tensors through the Transformer\n",
    "                encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "                decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "                proj_output = model.project(decoder_output)\n",
    "\n",
    "                # Loading the target labels onto the GPU\n",
    "                label = batch['label'].to(device)\n",
    "\n",
    "                # Computing loss between model's output and true labels\n",
    "                loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "\n",
    "                # Updating progress bar\n",
    "                batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "                writer.add_scalar('train loss', loss.item(), global_step)\n",
    "                writer.flush()\n",
    "\n",
    "                # Performing backpropagation\n",
    "                loss.backward()\n",
    "\n",
    "                # Updating parameters based on the gradients\n",
    "                optimizer.step()\n",
    "\n",
    "                # Clearing the gradients to prepare for the next batch\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                global_step += 1 # Updating global step count\n",
    "\n",
    "            # We run the 'run_validation' function at the end of each epoch\n",
    "            # to evaluate model performance\n",
    "            run_validation(model, valid_data_loader, tokenizer_src, tokenizer_tgt, config.seq_len, device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "            # Saving model\n",
    "            model_filename = self.get_weights_file_path(config, f'{epoch:02d}')\n",
    "            # Writting current model state to the 'model_filename'\n",
    "            torch.save({\n",
    "                'epoch': epoch, # Current epoch\n",
    "                'model_state_dict': model.state_dict(),# Current model state\n",
    "                'optimizer_state_dict': optimizer.state_dict(), # Current optimizer state\n",
    "                'global_step': global_step # Current global step\n",
    "            }, model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-28 10:30:09,009: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-05-28 10:30:09,022: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-05-28 10:30:09,031: INFO: common: created directory at: artifacts]\n",
      "[2024-05-28 10:30:09,035: INFO: common: created directory at: artifacts/trained_model]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     get_model_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_train_model_config()\n\u001b[0;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m modelTrainer(config\u001b[38;5;241m=\u001b[39mget_model_config)\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_model_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[29], line 95\u001b[0m, in \u001b[0;36mmodelTrainer.initiate_model_trainer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     90\u001b[0m tokenizer_tgt \u001b[38;5;241m=\u001b[39m Tokenizer\u001b[38;5;241m.\u001b[39mfrom_file(tokenizer_it)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Load the DataLoader objects\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m train_data_loader \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_loader_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m valid_data_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(valid_data_loader_path)\n\u001b[0;32m    100\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    808\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 809\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1170\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1171\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1172\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1174\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1141\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1142\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:1112\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[0;32m   1110\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1112\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m   1116\u001b[0m         wrap_storage\u001b[38;5;241m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1117\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1118\u001b[0m         _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    get_model_config = config.get_train_model_config()\n",
    "    model = modelTrainer(config=get_model_config)\n",
    "    model.initiate_model_trainer()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
