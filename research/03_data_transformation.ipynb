{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\lenovo\\\\Desktop\\\\Mini_Translator_2.0\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\lenovo\\\\Desktop\\\\Mini_Translator_2.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    tokenizer_file: Path\n",
    "    lang1:str\n",
    "    lang2:str\n",
    "\n",
    "    seq_len : int\n",
    "\n",
    "    data_loader:Path\n",
    "    batch_size: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Mini_Translator_T.constants import *\n",
    "from src.Mini_Translator_T.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        params=self.params\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            tokenizer_file=config.tokenizer_file,\n",
    "            lang1=params.lang1,\n",
    "            lang2=params.lang2,\n",
    "            seq_len=params.seq_len,\n",
    "            data_loader=config.data_loader,\n",
    "            batch_size=params.batch_size\n",
    "        )\n",
    "\n",
    "        return data_transformation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.Mini_Translator_T.logging import logger\n",
    "import spacy\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HuggingFace linraries\n",
    "# from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import json\n",
    "import json\n",
    "import os\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch\n",
    "from src.Mini_Translator_T.logging import logger  # Make sure to import the logger\n",
    "from typing import Any\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'artifacts\\data_ingestion\\raw\\raw.json', 'r') as json_file:\n",
    "#     ds_raw = json.load(json_file)[\"train\"]\n",
    "#     ds_raw=Dataset.from_pandas(pd.DataFrame(ds_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m         mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtriu(torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m, size, size), diagonal \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mint)\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBilingualDataset\u001b[39;00m(\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# This takes in the dataset contaning sentence pairs, the tokenizers for target and source languages, and the strings of source and target languages\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# 'seq_len' defines the sequence length for both languages\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def casual_mask(size):\n",
    "        # Creating a square matrix of dimensions 'size x size' filled with ones\n",
    "        mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n",
    "        return mask == 0\n",
    "class BilingualDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    # This takes in the dataset contaning sentence pairs, the tokenizers for target and source languages, and the strings of source and target languages\n",
    "    # 'seq_len' defines the sequence length for both languages\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        # Defining special tokens by using the target language tokenizer\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "\n",
    "    # Total number of instances in the dataset (some pairs are larger than others)\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    # Using the index to retrive source and target texts\n",
    "    def __getitem__(self, index: Any) -> Any:\n",
    "        src_target_pair = self.ds[index]\n",
    "\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        # Tokenizing source and target texts\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Computing how many padding tokens need to be added to the tokenized texts\n",
    "        # Source tokens\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 # Subtracting the two '[EOS]' and '[SOS]' special tokens\n",
    "        # Target tokens\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # Subtracting the '[SOS]' special token\n",
    "\n",
    "        # If the texts exceed the 'seq_len' allowed, it will raise an error. This means that one of the sentences in the pair is too long to be processed\n",
    "        # given the current sequence length limit (this will be defined in the config dictionary below)\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError('Sentence is too long')\n",
    "\n",
    "        # Building the encoder input tensor by combining several elements\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "            self.sos_token, # inserting the '[SOS]' token\n",
    "            torch.tensor(enc_input_tokens, dtype = torch.int64), # Inserting the tokenized source text\n",
    "            self.eos_token, # Inserting the '[EOS]' token\n",
    "            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Building the decoder input tensor by combining several elements\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token, # inserting the '[SOS]' token\n",
    "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
    "            ]\n",
    "\n",
    "        )\n",
    "\n",
    "        # Creating a label tensor, the expected output for training the model\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
    "                self.eos_token, # Inserting the '[EOS]' token\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Adding padding tokens\n",
    "\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Ensuring that the length of each tensor above is equal to the defined 'seq_len'\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            'encoder_input': encoder_input,\n",
    "            'decoder_input': decoder_input,\n",
    "            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),\n",
    "            'label': label,\n",
    "            'src_text': src_text,\n",
    "            'tgt_text': tgt_text\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open(r'artifacts\\data_ingestion\\validation.json', 'r') as json_file:\n",
    "#     val_ds_raw = json.load(json_file)\n",
    "\n",
    "# # Convert the loaded lists of dictionaries to datasets.Dataset objects\n",
    "# val_ds_raw = Dataset.from_pandas(pd.DataFrame(val_ds_raw))\n",
    "\n",
    "# val_ds_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_ds_raw['translation'][0]['en'].ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig, config_filepath=CONFIG_FILE_PATH):\n",
    "        self.config = config\n",
    "        \n",
    "        self.config2 = read_yaml(config_filepath)\n",
    "\n",
    "        # Defining Tokenizer\n",
    "\n",
    "    # Iterating through dataset to extract the original sentence and its translation\n",
    "    def get_all_sentences(self,ds, lang):\n",
    "        for pair in ds:\n",
    "            yield pair['translation'][lang]\n",
    "\n",
    "    def build_tokenizer(self,config, ds, lang):\n",
    "\n",
    "        # Crating a file path for the tokenizer\n",
    "        tokenizer_path = Path(config.format(lang))\n",
    "\n",
    "        # Checking if Tokenizer already exists\n",
    "        if not Path.exists(tokenizer_path):\n",
    "\n",
    "            # If it doesn't exist, we create a new one\n",
    "            tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]')) # Initializing a new world-level tokenizer\n",
    "            tokenizer.pre_tokenizer = Whitespace() # We will split the text into tokens based on whitespace\n",
    "\n",
    "            # Creating a trainer for the new tokenizer\n",
    "            trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\",\n",
    "                                                        \"[SOS]\", \"[EOS]\"], min_frequency = 2) # Defining Word Level strategy and special tokens\n",
    "\n",
    "            # Training new tokenizer on sentences from the dataset and language specified\n",
    "            tokenizer.train_from_iterator(self.get_all_sentences(ds, lang), trainer = trainer)\n",
    "            tokenizer.save(str(tokenizer_path)) # Saving trained tokenizer to the file path specified at the beginning of the function\n",
    "        else:\n",
    "            tokenizer = Tokenizer.from_file(str(tokenizer_path)) # If the tokenizer already exist, we load it\n",
    "        return tokenizer # Returns the loaded tokenizer or the trained tokenizer\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def initiate_tokenization(self):\n",
    "        ingestion_config = self.config2.data_ingestion\n",
    "\n",
    "        with open(ingestion_config.data_files.raw_data, 'r') as json_file:\n",
    "            ds_raw = json.load(json_file)[\"train\"]\n",
    "            ds_raw=Dataset.from_pandas(pd.DataFrame(ds_raw))\n",
    "\n",
    "        tokenizer_src = self.build_tokenizer(self.config.tokenizer_file, ds_raw, self.config.lang1)\n",
    "        tokenizer_tgt = self.build_tokenizer(self.config.tokenizer_file, ds_raw, self.config.lang2)\n",
    "\n",
    "        logger.info(['source tokenizerand target tokenizer saved succefully'])\n",
    "\n",
    "        with open(ingestion_config.data_files.train, 'r') as json_file:\n",
    "            train_ds_raw = json.load(json_file)\n",
    "\n",
    "        with open(ingestion_config.data_files.validation, 'r') as json_file:\n",
    "            val_ds_raw = json.load(json_file)\n",
    "\n",
    "\n",
    "\n",
    "        # Convert the loaded lists of dictionaries to datasets.Dataset objects\n",
    "        train_ds_raw = Dataset.from_pandas(pd.DataFrame(train_ds_raw))\n",
    "        val_ds_raw = Dataset.from_pandas(pd.DataFrame(val_ds_raw))\n",
    "\n",
    "\n",
    "       # Processing data with the BilingualDataset class, which we will define below\n",
    "        train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, self.config.lang1, self.config.lang2, self.config.seq_len)\n",
    "        val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, self.config.lang1, self.config.lang2, self.config.seq_len)\n",
    "\n",
    "\n",
    "        \n",
    "        # Iterating over the entire dataset and printing the maximum length found in the sentences of both the source and target languages\n",
    "        max_len_src = 0\n",
    "        max_len_tgt = 0\n",
    "        for pair in ds_raw:\n",
    "                src_ids = tokenizer_src.encode(pair['translation'][self.config.lang1]).ids\n",
    "                tgt_ids = tokenizer_src.encode(pair['translation'][self.config.lang2]).ids\n",
    "                max_len_src = max(max_len_src, len(src_ids))\n",
    "                max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "        print(src_ids)\n",
    "        logger.info(f'Max length of source sentence: {max_len_src}')\n",
    "        logger.info(f'Max length of target sentence: {max_len_tgt}')\n",
    "        \n",
    "\n",
    "            # Creating dataloaders for the training and validadion sets\n",
    "    # Dataloaders are used to iterate over the dataset in batches during training and validation\n",
    "        train_dataloader = DataLoader(train_ds, batch_size = self.config.batch_size, shuffle = True) # Batch size will be defined in the config dictionary\n",
    "        val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n",
    "\n",
    "        logger.info(f\"Length of train data loader: {len(train_dataloader)}\")\n",
    "        logger.info(f\"Length of valid data loader: {len(val_dataloader)}\")\n",
    "\n",
    "        # Saving dataloaders as list of batches\n",
    "        root_dir=self.config.root_dir\n",
    "        torch.save(list(train_dataloader), os.path.join(root_dir, \"train_data_loader.pth\"))  # Changed line\n",
    "        torch.save(list(val_dataloader), os.path.join(root_dir, \"valid_data_loader.pth\"))  # Changed line\n",
    "\n",
    "        logger.info(f\"Data loaders saved to {root_dir}\")\n",
    "        logger.info(\"Data transformation successfully completed\")\n",
    "\n",
    "        # return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt # Returning the DataLoader objects and tokenizers\n",
    " \n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-26 12:27:08,252: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-05-26 12:27:08,264: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-05-26 12:27:08,267: INFO: common: created directory at: artifacts]\n",
      "[2024-05-26 12:27:08,271: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2024-05-26 12:27:08,287: INFO: common: yaml file: config\\config.yaml loaded successfully]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-26 12:27:09,019: INFO: 4083326783: ['source tokenizerand target tokenizer saved succefully']]\n",
      "[12, 9, 204, 162, 171, 923, 22, 3743, 5, 1055, 13, 5, 189, 130, 4, 204, 2521, 13, 5, 189, 130, 4, 204, 14164, 1018, 28, 326, 18, 64, 77, 162, 37, 11, 726, 321, 28, 427, 12, 41, 4063, 10, 11051, 6, 118, 197, 18, 191, 28, 230, 9, 204, 162, 1304, 26, 28, 160, 3108, 6, 204, 3035, 10, 17, 7, 370, 515, 77, 162, 21, 269, 305, 9, 1794, 4, 31, 9, 204, 162, 1794, 4, 6, 28, 150, 4, 28, 271, 150, 4, 11104, 10, 249, 15, 232, 1021, 8, 33, 4, 34, 220, 250, 10, 17, 67, 435, 11239, 25, 17, 14, 106, 4, 31, 126, 65, 7566, 830, 10, 1752, 22, 43, 9, 40, 5, 605, 8, 14204, 17, 76]\n",
      "[2024-05-26 12:27:23,632: INFO: 4083326783: Max length of source sentence: 309]\n",
      "[2024-05-26 12:27:23,632: INFO: 4083326783: Max length of target sentence: 274]\n",
      "[2024-05-26 12:27:23,637: INFO: 4083326783: Length of train data loader: 3638]\n",
      "[2024-05-26 12:27:23,637: INFO: 4083326783: Length of valid data loader: 3234]\n",
      "[2024-05-26 12:57:17,670: INFO: 4083326783: Data loaders saved to artifacts/data_transformation]\n",
      "[2024-05-26 12:57:18,862: INFO: 4083326783: Data transformation successfully completed]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.initiate_tokenization()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
